{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Data Science template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1639606181816
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries and packages\n",
    "\n",
    "from pydoc import help  \n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pylab import rcParams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV   \n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import your data and start exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639606228222
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Read csv file into pandas dataframe\n",
    "\n",
    "df = pd.read_csv(\"PutYourFileNameHere.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639606281699
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get dataset info\n",
    "\n",
    "print(\"############################################\")\n",
    "print(\"          Info Of the Data Set\")\n",
    "print(\"############################################\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring distribution\n",
    "\n",
    "fig, ax = plt.subplots(6, 2, figsize = (15, 13))\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[0,0])\n",
    "sns.distplot(df['var'], ax = ax[0,1])\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[1,0])\n",
    "sns.distplot(df['var'], ax = ax[1,1])\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[2,0])\n",
    "sns.distplot(df['var'], ax = ax[2,1])\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[3,0])\n",
    "sns.distplot(df['var'], ax = ax[3,1])\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[4,0])\n",
    "sns.distplot(df['var'], ax = ax[4,1])\n",
    "sns.boxplot(x= df[\"var\"], ax = ax[5,0])\n",
    "sns.distplot(df['var'], ax = ax[5,1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639606370162
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a function that we can re-use\n",
    "\n",
    "def show_distribution(var_data):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    # Get statistics\n",
    "    min_val = var_data.min()\n",
    "    max_val = var_data.max()\n",
    "    mean_val = var_data.mean()\n",
    "    med_val = var_data.median()\n",
    "    mod_val = var_data.mode()[0]\n",
    "\n",
    "    print('Minimum:{:.2f}\\nMean:{:.2f}\\nMedian:{:.2f}\\nMode:{:.2f}\\nMaximum:{:.2f}\\n'.format(min_val,\n",
    "                                                                                            mean_val,\n",
    "                                                                                            med_val,\n",
    "                                                                                            mod_val,\n",
    "                                                                                            max_val))\n",
    "\n",
    "    # Create a figure for 2 subplots (2 rows, 1 column)\n",
    "    fig, ax = plt.subplots(2, 1, figsize = (10,4))\n",
    "\n",
    "    # Plot the histogram   \n",
    "    ax[0].hist(var_data)\n",
    "    ax[0].set_ylabel('Frequency')\n",
    "\n",
    "    # Add lines for the mean, median, and mode\n",
    "    ax[0].axvline(x=min_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=mean_val, color = 'cyan', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=med_val, color = 'red', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=mod_val, color = 'yellow', linestyle='dashed', linewidth = 2)\n",
    "    ax[0].axvline(x=max_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
    "\n",
    "    # Plot the boxplot   \n",
    "    ax[1].boxplot(var_data, vert=False)\n",
    "    ax[1].set_xlabel('Value')\n",
    "\n",
    "    # Add a title to the Figure\n",
    "    fig.suptitle('Data Distribution')\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Get the variable to examine\n",
    "col = df['VarName']\n",
    "# Call the function\n",
    "show_distribution(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1635889558205
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# heat map\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(),cmap=plt.cm.Reds,annot=True)\n",
    "plt.title('Heatmap displaying the relationship between the features of the data',\n",
    "         fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1635889800274
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Exploring relationships - this code will create scatter plots \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "  \n",
    "# selecting numerical features\n",
    "features = ['Var1', 'Var2', 'Var3', 'Var4']\n",
    "   \n",
    "# plotting the scatter matrix with the features\n",
    "scatter_matrix(df[features], figsize=(12,12))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# If you want to compare groups based on one or more quantitative variables, use this code\n",
    "\n",
    "col_list = ['GroupingVarName', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6']\n",
    "short_df = df[col_list]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "rs=1999\n",
    " \n",
    "df_long = pd.melt(short_df.sample(1000,random_state=rs), \"GroupingVarName\", var_name=\"Columns\", value_name=\"Values\")   \n",
    "f,ax = plt.subplots(figsize=(16,8))\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.ylim(0, 10)\n",
    "#plt.xlim(0, None)\n",
    "sns.boxplot(x=\"Columns\", y=\"Values\", hue=\"GroupingVarName\", data=df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1['Name 1'] = df['var1']\n",
    "df1['Name 2'] = df['var2']\n",
    "df1['Name 3'] = df['var3']\n",
    "df1['Name'] = df['CatVar']\n",
    "boxplot = df1.boxplot(column=['Name 1', 'Name 2', 'Name 3'], by=['Name'], rot=45, fontsize=10, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value handling\n",
    "\n",
    "meanx = df['var'].mean()\n",
    "df['var'] = df['var'].fillna(meanx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normality check\n",
    "\n",
    "print(\"Skewness: %f\" %df['var'].skew())\n",
    "print(\"Kurtosis: %f\" %df['var'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1635890269030
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Removing outliers - Use if you want to get rid of outliers and rerun the matrix\n",
    "# You can add more lines for additional variables\n",
    "# Typically we use 3 standard deviations (e.g., Six Sigma - 3 SDs above and below mean)\n",
    "\n",
    "#Create a new dataset (df2) so that you don't replace the values in df\n",
    "df2=copy.df()\n",
    "\n",
    "from scipy import stats\n",
    "df2=df2[(np.abs(stats.zscore(df2['Var1'])) < 3)]\n",
    "df2=df2[(np.abs(stats.zscore(df2['Var2'])) < 3)]\n",
    "df2=df2[(np.abs(stats.zscore(df2['Var3'])) < 3)]\n",
    "df2=df2[(np.abs(stats.zscore(df2['Var4'])) < 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636055724700
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use this if doing normalization - setting the scale to be between 0 and 1\n",
    "# You would only do this if you want to standardize your variables and have all of them on same scale\n",
    "# This would be preferred over creating z scores if your distribution is not normal\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Again, creating a new dataset so we don't replace the values in the original dataset\n",
    "df2=df.copy()\n",
    "\n",
    "# Applying scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "num_vars = ['Var1', 'Var2', 'Var3', 'Var4']\n",
    "df2[num_vars] = scaler.fit_transform(df2[num_vars])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use this if doing standardization (z scores) - you would do this or the one above, but not both\n",
    "# This code creates a z score that has zero as the mean. It is different than normalization\n",
    "\n",
    "df['Var1'] = (df['Var1'] - df['Var1'].mean())/df['Var1'].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode categorical variables\n",
    "\n",
    "df['var'] = df['var'].replace({1: 'name', 2: 'name', 3: 'name'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy variable\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['var'], prefix=None)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636553225115
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Run this code if you get errors about infinity values or missing values and rerun time series\n",
    "# Note that this can cause you to end up with no values so you may need to try to transform, etc. instead of simply deleting\n",
    "\n",
    "# Replace Inf values with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "# Drop all occurences of NaN\n",
    "df = df.dropna()\n",
    "# Double check these are all gone\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636556361081
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Decomposition model to get seasonality, trend and noise\n",
    "# This code creates a time series with 3 components (seasonal, trend, and noise)\n",
    "# Change Var1 with your variable of interest. Change df to your dataset name if it is different\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result=seasonal_decompose(df['Var1'], model='additive', freq=12, extrapolate_trend=12)\n",
    "\n",
    "\n",
    "result.plot()\n",
    "# pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636556369148
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Run this code to save the three components so you can download to csv file. \n",
    "\n",
    "df['trend'] = result.trend\n",
    "\n",
    "df['residual'] = result.resid\n",
    "\n",
    "df['seasonal'] = result.seasonal\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use this code to save your dataset to a csv file that will load into this folder\n",
    "\n",
    "df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636553362180
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is code to run a linear regression analysis\n",
    "# Change df to be your dataset name if it is different\n",
    "# Change Target to be your dependent variable\n",
    "# Change Var1, Var2, Var3, Var4 to be the name of your predictor variables\n",
    "# You can add more predictors if you want\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "\n",
    "new_model = ols(\"\"\"Target ~ Var1 \n",
    "                                     + Var2 \n",
    "                                     + Var3\n",
    "                                     + Var4\"\"\", data=df).fit()\n",
    "# summarize our model\n",
    "new_model_summary = new_model.summary()\n",
    "HTML(new_model_summary.as_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1636556414182
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " #This code block runs regression diagnostic visualizations (partial plots)\n",
    "\n",
    "fig = plt.figure(figsize=(20,12))\n",
    "fig = sm.graphics.plot_partregress_grid(new_model, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This produces our four regression plots for specified feature variable (e.g., one of your predictor variables)\n",
    "# You would run this code for each predictor variable\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "\n",
    "# pass in the model as the first parameter, then specify the \n",
    "# predictor variable we want to analyze\n",
    "fig = sm.graphics.plot_regress_exog(new_model, \"Var1\", fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Decision trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of features for the decision tree\n",
    "# Include in this list, all the predictor variables that you want to use in your decision tree\n",
    "\n",
    "features=['Var1',\n",
    " 'Var2',\n",
    " 'Var3',\n",
    " 'Var4',\n",
    " 'Var5',\n",
    " 'Var6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['var'].notna()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn import datasets, tree\n",
    "\n",
    "y = size[target]\n",
    "X = size[features]#.astype(float)\n",
    "dt = DecisionTreeRegressor(min_samples_split=100, min_samples_leaf=100, random_state=99, max_depth=10, max_leaf_nodes=10)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = tree.export_text(dt)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "with open(\"decistion_tree.log\", \"w\") as fout:\n",
    "    fout.write(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt,\n",
    "                   feature_names=features,\n",
    "                   class_names=target,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LazyPredict Classifer - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Example\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df\n",
    "X = data.data\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove date variables (correlation b/w region and store will be addressed in differrent models )\n",
    "df.drop(['var1','var2','var3','var4'], axis=1, inplace=True) \n",
    "\n",
    "#train test split\n",
    "train_size = int(len(df1.index) * 0.9)\n",
    "train, test = df1[0:train_size], df1[train_size:len(df1.index)]\n",
    "print(len(df1.index))\n",
    "print(len(train))\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Features Scaling\n",
    "# train = (train - train.mean()) / train.std()\n",
    "# test = (test - test.mean()) / test.std()\n",
    "\n",
    "X_train, y_train = train.drop(columns=[\"TargetVar\"], axis=1), train[\"TargetVar\"]\n",
    "X_test, y_test = test.drop(columns=[\"TargetVar\"], axis=1), test[\"TargetVar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_train = xgb.DMatrix(X_train, label = y_train)\n",
    "matrix_test = xgb.DMatrix(X_test, label = y_test)\n",
    "\n",
    "# Run XGB \n",
    "model = xgb.train(params={'objective':'reg:squarederror','eval_metric':'mae'}\n",
    "                ,dtrain = matrix_train, num_boost_round = 500, \n",
    "                early_stopping_rounds = 20, evals = [(matrix_test,'test')],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_score)\n",
    "print(model.best_iteration)\n",
    "print(\"Best tree limit:\", model.best_ntree_limit)\n",
    "\n",
    "y_pred = model.predict(matrix_test, model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "xgb.plot_importance(model, height=0.5, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_pred = test.copy()\n",
    "df_pred['Forecast'] = y_pred\n",
    "# Plot Actual vs. Forecast for a given store \n",
    "sample = random.choice(df_pred.store_id.unique())\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(df_pred[df_pred.store_id == sample]['TargetVar'],label='Actual')\n",
    "plt.plot(df_pred[df_pred.store_id == sample]['Forecast'], \n",
    "         color='red', label='Forecast')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans - use K prototype for mixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "KMeans(init='random', n_clusters=10, n_init=10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize kmeans parameters\n",
    "kmeans_kwargs = {\n",
    "\"init\": \"random\",\n",
    "\"n_init\": 10,\n",
    "\"random_state\": 1,\n",
    "}\n",
    "\n",
    "#create list to hold SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(df)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "#visualize results\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(df)\n",
    "X_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_scaled.T\n",
    "cov_matrix = np.cov(features)\n",
    "cov_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix2 = np.nan_to_num(cov_matrix, copy=True, nan=0.0, posinf=30, neginf=-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(cov_matrix2)\n",
    "values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_1 = X_scaled.dot(vectors.T[0])\n",
    "projected_2 = X_scaled.dot(vectors.T[1])\n",
    "projected_3 = X_scaled.dot(vectors.T[2])\n",
    "y = df['CatVar1']\n",
    "z = df['CatVar2']\n",
    "res = pd.DataFrame(projected_1, columns=['PC1'])\n",
    "res['PC2'] = projected_2\n",
    "res['PC3'] = projected_3\n",
    "res['CatVar1'] = y\n",
    "res['CatVar2'] = z\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(data=res, x=\"PC1\", y=\"PC2\", hue=\"CatVar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(data=res, x=\"PC1\", y=\"PC2\", hue=\"CatVar2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('df.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('TargetVar',axis=1), \n",
    "                                                    train['TargetVar'], test_size=0.30, \n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
